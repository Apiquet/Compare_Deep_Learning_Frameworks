{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook can be used in Google Colab\n",
        "\n",
        "It implements a CIFAR10 training using JAX Deep Learning framework"
      ],
      "metadata": {
        "id": "N-G7aU1jYcw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This notebook is inspired from JAX mnist example\n",
        "# https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html"
      ],
      "metadata": {
        "id": "AEUhvygLYhoI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Ensure TF does not see GPU and grab all GPU memory.\n",
        "tf.config.set_visible_devices([], device_type=\"GPU\")\n",
        "\n",
        "\n",
        "def get_data_from_tfds(\n",
        "    name: str,\n",
        "    data_dir: Path,\n",
        ") -> Tuple[tf.Tensor | tf.data.Dataset, tf.Tensor | tf.data.Dataset]:\n",
        "    \"\"\"Fetch full datasets for evaluation.\n",
        "\n",
        "    Args:\n",
        "        name: name of the dataset for tfds.load() method\n",
        "        data_dir: path to save the data\n",
        "    Returns:\n",
        "        tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
        "    \"\"\"\n",
        "    cifar10_data, _ = tfds.load(\n",
        "        name=name,\n",
        "        batch_size=-1,\n",
        "        data_dir=data_dir,\n",
        "        with_info=True,\n",
        "    )\n",
        "    cifar10_data = tfds.as_numpy(cifar10_data)\n",
        "    train_data, test_data = cifar10_data[\"train\"], cifar10_data[\"test\"]\n",
        "    return train_data, test_data\n"
      ],
      "metadata": {
        "id": "DhOfE5WXp2aq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax.example_libraries import stax, optimizers\n",
        "\n",
        "train_data, test_data = get_data_from_tfds(name=\"cifar10\", data_dir=Path('/tmp/tfds'))\n",
        "\n",
        "X_train, Y_train = train_data['image'], train_data['label']\n",
        "X_test, Y_test = test_data['image'], test_data['label']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = jnp.array(X_train, dtype=jnp.float32),\\\n",
        "                                   jnp.array(X_test, dtype=jnp.float32),\\\n",
        "                                   jnp.array(Y_train, dtype=jnp.float32),\\\n",
        "                                   jnp.array(Y_test, dtype=jnp.float32)\n",
        "classes =  jnp.unique(Y_train)\n",
        "conv_init, conv_apply = stax.serial(\n",
        "    stax.Conv(32, (3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool(window_shape=(2, 2), strides=(2, 2)),\n",
        "    stax.Conv(16, (3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool(window_shape=(2, 2), strides=(2, 2)),\n",
        "    stax.Flatten,\n",
        "    stax.Dense(64),\n",
        "    stax.Relu,\n",
        "    stax.Dense(len(classes)),\n",
        "    stax.Softmax\n",
        ")"
      ],
      "metadata": {
        "id": "tJF-qlv-oK7O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init weights and verify the shapes\n",
        "import jax\n",
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = conv_init(rng, (18,32,32,3))[1]\n",
        "\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9rcRN6DnbSL",
        "outputId": "fbaa932d-e934-4a19-bb1c-36a5f034cfe8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights : (3, 3, 3, 32), Biases : (1, 1, 1, 32)\n",
            "Weights : (3, 3, 32, 16), Biases : (1, 1, 1, 16)\n",
            "Weights : (1024, 64), Biases : (64,)\n",
            "Weights : (64, 10), Biases : (10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction and verify output shape\n",
        "preds = conv_apply(weights, X_train[:5])\n",
        "preds.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-Y_nQlIncX6",
        "outputId": "b2815110-dcfd-4f8e-83bd-5662ca5612b7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "\n",
        "def CrossEntropyLoss(\n",
        "    conv_apply: Callable,\n",
        "    weights: list,\n",
        "    input_data: jax.Array,\n",
        "    targets: jax.Array,\n",
        ") -> jax.Array:\n",
        "    \"\"\"Implement of cross entropy loss.\n",
        "\n",
        "    Args:\n",
        "        conv_apply: callable from _, conv_apply = stax.serial(...)\n",
        "        weights: list from _, _, opt_get_weights = optimizers.adam(lr), opt_get_weights(opt_state)\n",
        "        input_data: data to predict\n",
        "        targets: groundtruth targets\n",
        "\n",
        "    Returns:\n",
        "        loss value\n",
        "    \"\"\"\n",
        "    preds = conv_apply(weights, input_data)\n",
        "    log_preds = jnp.log(preds + tf.keras.backend.epsilon())\n",
        "    return -jnp.mean(targets * log_preds)"
      ],
      "metadata": {
        "id": "-DHbTGDxqkWW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-lXqj9Qnq9Gp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}