{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-G7aU1jYcw4"
      },
      "source": [
        "This notebook can be used in Google Colab\n",
        "\n",
        "It implements a CIFAR10 training using JAX Deep Learning framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEUhvygLYhoI"
      },
      "outputs": [],
      "source": [
        "# This notebook is inspired from JAX mnist example\n",
        "# https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhOfE5WXp2aq"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Ensure TF does not see GPU and grab all GPU memory.\n",
        "tf.config.set_visible_devices([], device_type=\"GPU\")\n",
        "\n",
        "\n",
        "def get_data_from_tfds(\n",
        "    name: str,\n",
        "    data_dir: Path,\n",
        ") -> Tuple[tf.Tensor | tf.data.Dataset, tf.Tensor | tf.data.Dataset]:\n",
        "    \"\"\"Fetch full datasets for evaluation.\n",
        "\n",
        "    Args:\n",
        "        name: name of the dataset for tfds.load() method\n",
        "        data_dir: path to save the data\n",
        "    Returns:\n",
        "        tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
        "    \"\"\"\n",
        "    cifar10_data, _ = tfds.load(\n",
        "        name=name,\n",
        "        batch_size=-1,\n",
        "        data_dir=data_dir,\n",
        "        with_info=True,\n",
        "    )\n",
        "    cifar10_data = tfds.as_numpy(cifar10_data)\n",
        "    train_data, test_data = cifar10_data[\"train\"], cifar10_data[\"test\"]\n",
        "    return train_data, test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJF-qlv-oK7O"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax.example_libraries import stax, optimizers\n",
        "\n",
        "train_data, test_data = get_data_from_tfds(name=\"cifar10\", data_dir=Path('/tmp/tfds'))\n",
        "\n",
        "X_train, Y_train = train_data['image'], train_data['label']\n",
        "X_test, Y_test = test_data['image'], test_data['label']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = jnp.array(X_train, dtype=jnp.float32),\\\n",
        "                                   jnp.array(X_test, dtype=jnp.float32),\\\n",
        "                                   jnp.array(Y_train, dtype=jnp.float32),\\\n",
        "                                   jnp.array(Y_test, dtype=jnp.float32)\n",
        "classes =  jnp.unique(Y_train)\n",
        "conv_init, conv_apply = stax.serial(\n",
        "    stax.Conv(32, (3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool(window_shape=(2, 2), strides=(2, 2)),\n",
        "    stax.Conv(16, (3,3), padding=\"SAME\"),\n",
        "    stax.Relu,\n",
        "    stax.MaxPool(window_shape=(2, 2), strides=(2, 2)),\n",
        "    stax.Flatten,\n",
        "    stax.Dense(64),\n",
        "    stax.Relu,\n",
        "    stax.Dense(len(classes)),\n",
        "    stax.Softmax\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9rcRN6DnbSL",
        "outputId": "fbaa932d-e934-4a19-bb1c-36a5f034cfe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights : (3, 3, 3, 32), Biases : (1, 1, 1, 32)\n",
            "Weights : (3, 3, 32, 16), Biases : (1, 1, 1, 16)\n",
            "Weights : (1024, 64), Biases : (64,)\n",
            "Weights : (64, 10), Biases : (10,)\n"
          ]
        }
      ],
      "source": [
        "# Init weights and verify the shapes\n",
        "import jax\n",
        "rng = jax.random.PRNGKey(123)\n",
        "\n",
        "weights = conv_init(rng, (18,32,32,3))[1]\n",
        "\n",
        "for w in weights:\n",
        "    if w:\n",
        "        w, b = w\n",
        "        print(\"Weights : {}, Biases : {}\".format(w.shape, b.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-Y_nQlIncX6",
        "outputId": "b2815110-dcfd-4f8e-83bd-5662ca5612b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 10)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make a prediction and verify output shape\n",
        "preds = conv_apply(weights, X_train[:5])\n",
        "preds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-DHbTGDxqkWW"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "def CrossEntropyLoss(\n",
        "    weights: list,\n",
        "    input_data: jax.Array,\n",
        "    targets: jax.Array,\n",
        ") -> jax.Array:\n",
        "    \"\"\"Implement of cross entropy loss.\n",
        "\n",
        "    Args:\n",
        "        weights: list from _, _, opt_get_weights = optimizers.adam(lr), opt_get_weights(opt_state)\n",
        "        input_data: data to predict\n",
        "        targets: groundtruth targets in one hot encoding\n",
        "\n",
        "    Returns:\n",
        "        loss value\n",
        "    \"\"\"\n",
        "    preds = conv_apply(weights, input_data)\n",
        "    log_preds = jnp.log(preds + tf.keras.backend.epsilon())\n",
        "    return -jnp.mean(targets * log_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-lXqj9Qnq9Gp"
      },
      "outputs": [],
      "source": [
        "from jax import value_and_grad\n",
        "from tqdm import tqdm\n",
        "\n",
        "def TrainModelInBatches(\n",
        "    X: jax.Array,\n",
        "    Y: jax.Array,\n",
        "    epochs: int,\n",
        "    opt_state: jax.example_libraries.optimizers.OptimizerState,\n",
        "    opt_update: Callable,\n",
        "    opt_get_weights: Callable,\n",
        "    batch_size: int,\n",
        ") -> jax.example_libraries.optimizers.OptimizerState:\n",
        "    \"\"\"Train Jax model in batches.\n",
        "\n",
        "    Args:\n",
        "        X: training input\n",
        "        Y: groundtruth in one hot encoding\n",
        "        epochs: number of epochs\n",
        "        opt_state: from opt_init(weights)\n",
        "        opt_update: from _, opt_update, _ = optimizers.adam(lr)\n",
        "        opt_get_weights: from _, _, opt_get_weights = optimizers.adam(lr)\n",
        "        batch_size: batch size for training\n",
        "\n",
        "    Returns:\n",
        "        updated opt_state\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(epochs):\n",
        "        batches = jnp.arange((X.shape[0] // batch_size) + 1)\n",
        "        progress_bar = tqdm(batches, position=0, leave=True)\n",
        "\n",
        "        losses = []\n",
        "        for batch in batches:\n",
        "            if batch != batches[-1]:\n",
        "                start, end = int(batch * batch_size), int(batch * batch_size + batch_size)\n",
        "            else:\n",
        "                start, end = int(batch * batch_size), None\n",
        "\n",
        "            X_batch, Y_batch = X[start:end], Y[start:end]\n",
        "\n",
        "            loss, gradients = value_and_grad(CrossEntropyLoss)(\n",
        "                opt_get_weights(opt_state),\n",
        "                X_batch,\n",
        "                Y_batch,\n",
        "            )\n",
        "\n",
        "            ## Update Weights\n",
        "            opt_state = opt_update(i, gradients, opt_state)\n",
        "\n",
        "            losses.append(loss)\n",
        "\n",
        "            progress_bar.set_description(f\"Epoch {i+1}/{epochs}\")\n",
        "            progress_bar.set_postfix(train_loss=jnp.round(jnp.array(losses).mean(), decimals=3))\n",
        "            progress_bar.update()\n",
        "\n",
        "    return opt_state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = jax.random.PRNGKey(42)\n",
        "learning_rate = jnp.array(1/1e4)\n",
        "epochs = 2\n",
        "\n",
        "batch_size=256\n",
        "\n",
        "opt_init, opt_update, opt_get_weights = optimizers.adam(learning_rate)\n",
        "opt_state = opt_init(weights)\n",
        "one_hot_targets = jax.nn.one_hot(Y_train, num_classes=len(classes))\n",
        "\n",
        "\n",
        "final_opt_state = TrainModelInBatches(\n",
        "    X=X_train,\n",
        "    Y=one_hot_targets,\n",
        "    epochs=epochs,\n",
        "    opt_state=opt_state,\n",
        "    opt_update=opt_update,\n",
        "    opt_get_weights=opt_get_weights,\n",
        "    batch_size=batch_size,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bAlHHEoxdxf",
        "outputId": "10489cf6-2275-4008-ed69-80394b9c5e94"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/196 [11:01<?, ?it/s]\n",
            "Epoch 1/2:   2%|▏         | 3/196 [05:14<5:36:48, 104.71s/it, train_loss=1.3820001]\n",
            "Epoch 1/2: 100%|██████████| 196/196 [03:12<00:00,  1.02it/s, train_loss=0.95400006]\n",
            "Epoch 2/2: 100%|██████████| 196/196 [02:56<00:00,  1.11it/s, train_loss=0.26000002]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xx5KyUmGydNE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}