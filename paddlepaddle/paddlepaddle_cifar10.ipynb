{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyWG4I4kpDv_",
        "outputId": "52da566d-233a-4a2b-debd-fd7d4624e77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting paddlepaddle\n",
            "  Downloading paddlepaddle-2.4.2-cp310-cp310-manylinux1_x86_64.whl (121.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.7/121.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (1.22.4)\n",
            "Collecting protobuf<=3.20.0,>=3.1.0 (from paddlepaddle)\n",
            "  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (8.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (1.16.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (4.4.2)\n",
            "Collecting astor (from paddlepaddle)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting paddle-bfloat==0.1.7 (from paddlepaddle)\n",
            "  Downloading paddle_bfloat-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (3.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->paddlepaddle) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->paddlepaddle) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->paddlepaddle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->paddlepaddle) (3.4)\n",
            "Installing collected packages: paddle-bfloat, protobuf, astor, paddlepaddle\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery 3.9.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.19.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.15.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed astor-0.8.1 paddle-bfloat-0.1.7 paddlepaddle-2.4.2 protobuf-3.20.0\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install paddlepaddle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XFDL7fi_2-Vo"
      },
      "outputs": [],
      "source": [
        "# Code from the official documentation https://github.com/PaddlePaddle/book/blob/develop/03.image_classification/README.md\n",
        "from __future__ import print_function\n",
        "import paddle\n",
        "import paddle.fluid as fluid\n",
        "import numpy\n",
        "import sys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z3AQOufRmuz9"
      },
      "outputs": [],
      "source": [
        "def vgg_bn_drop(input):\n",
        "    def conv_block(ipt, num_filter, groups, dropouts):\n",
        "        return fluid.nets.img_conv_group(\n",
        "            input=ipt,\n",
        "            pool_size=2,\n",
        "            pool_stride=2,\n",
        "            conv_num_filter=[num_filter] * groups,\n",
        "            conv_filter_size=3,\n",
        "            conv_act='relu',\n",
        "            conv_with_batchnorm=True,\n",
        "            conv_batchnorm_drop_rate=dropouts,\n",
        "            pool_type='max')\n",
        "\n",
        "    conv1 = conv_block(input, 64, 2, [0.3, 0])\n",
        "    conv2 = conv_block(conv1, 128, 2, [0.4, 0])\n",
        "    conv3 = conv_block(conv2, 256, 3, [0.4, 0.4, 0])\n",
        "    conv4 = conv_block(conv3, 512, 3, [0.4, 0.4, 0])\n",
        "    conv5 = conv_block(conv4, 512, 3, [0.4, 0.4, 0])\n",
        "\n",
        "    drop = fluid.layers.dropout(x=conv5, dropout_prob=0.5)\n",
        "    fc1 = fluid.layers.fc(input=drop, size=512, act=None)\n",
        "    bn = fluid.layers.batch_norm(input=fc1, act='relu')\n",
        "    drop2 = fluid.layers.dropout(x=bn, dropout_prob=0.5)\n",
        "    fc2 = fluid.layers.fc(input=drop2, size=512, act=None)\n",
        "    predict = fluid.layers.fc(input=fc2, size=10, act='softmax')\n",
        "    return predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZzvF6tH7Y8bI"
      },
      "outputs": [],
      "source": [
        "def inference_program():\n",
        "    # The image is 32 * 32 with RGB representation.\n",
        "    data_shape = [3, 32, 32]\n",
        "    images = fluid.layers.data(name='pixel', shape=data_shape, dtype='float32')\n",
        "\n",
        "    predict = vgg_bn_drop(images)\n",
        "    # predict = vgg_bn_drop(images) # un-comment to use vgg net\n",
        "    return predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "emWH4An7ZNZQ"
      },
      "outputs": [],
      "source": [
        "def train_program():\n",
        "    predict = inference_program()\n",
        "\n",
        "    label = fluid.layers.data(name='label', shape=[1], dtype='int64')\n",
        "    cost = fluid.layers.cross_entropy(input=predict, label=label)\n",
        "    avg_cost = fluid.layers.mean(cost)\n",
        "    accuracy = fluid.layers.accuracy(input=predict, label=label)\n",
        "    return [avg_cost, accuracy, predict]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w9edDvsfZQHE"
      },
      "outputs": [],
      "source": [
        "def optimizer_program():\n",
        "    return fluid.optimizer.Adam(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yDQOXcJAZT6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6fe59c5-35b8-4199-f4d0-ea0aeb794b00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cache file /root/.cache/paddle/dataset/cifar/cifar-10-python.tar.gz not found, downloading https://dataset.bj.bcebos.com/cifar/cifar-10-python.tar.gz \n",
            "Begin to download\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "item 41626/41626 [============================>.] - ETA: 0s - 2ms/item"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Download finished\n"
          ]
        }
      ],
      "source": [
        "# Each batch will yield 128 images\n",
        "batch_size = 128\n",
        "buf_size = 50000\n",
        "# Reader for training\n",
        "train_reader = paddle.batch(\n",
        "    paddle.reader.shuffle(paddle.dataset.cifar.train10(), buf_size=buf_size),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "# Reader for testing. A separated data set for testing.\n",
        "test_reader = paddle.batch(\n",
        "    paddle.dataset.cifar.test10(), batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "igJQCZYUZVYw"
      },
      "outputs": [],
      "source": [
        "use_cuda = False\n",
        "place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
        "paddle.enable_static()\n",
        "\n",
        "feed_order = ['pixel', 'label']\n",
        "\n",
        "main_program = fluid.default_main_program()\n",
        "star_program = fluid.default_startup_program()\n",
        "\n",
        "avg_cost, acc, predict = train_program()\n",
        "\n",
        "# Test program\n",
        "test_program = main_program.clone(for_test=True)\n",
        "\n",
        "optimizer = optimizer_program()\n",
        "optimizer.minimize(avg_cost)\n",
        "\n",
        "exe = fluid.Executor(place)\n",
        "\n",
        "EPOCH_NUM = 2\n",
        "\n",
        "# For training test cost\n",
        "def train_test(program, reader):\n",
        "    count = 0\n",
        "    feed_var_list = [\n",
        "        program.global_block().var(var_name) for var_name in feed_order\n",
        "    ]\n",
        "    feeder_test = fluid.DataFeeder(\n",
        "        feed_list=feed_var_list, place=place)\n",
        "    test_exe = fluid.Executor(place)\n",
        "    accumulated = len([avg_cost, acc]) * [0]\n",
        "    for tid, test_data in enumerate(reader()):\n",
        "        avg_cost_np = test_exe.run(program=program,\n",
        "                                   feed=feeder_test.feed(test_data),\n",
        "                                   fetch_list=[avg_cost, acc])\n",
        "        accumulated = [x[0] + x[1][0] for x in zip(accumulated, avg_cost_np)]\n",
        "        count += 1\n",
        "    return [x / count for x in accumulated]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OUCc-09oZW71"
      },
      "outputs": [],
      "source": [
        "params_dirname = \"image_classification_resnet.inference.model\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "train_prompt = \"Train cost\"\n",
        "test_prompt = \"Test cost\"\n",
        "\n",
        "# main train loop.\n",
        "def train_loop():\n",
        "    feed_var_list_loop = [\n",
        "        main_program.global_block().var(var_name) for var_name in feed_order\n",
        "    ]\n",
        "    feeder = fluid.DataFeeder(\n",
        "        feed_list=feed_var_list_loop, place=place)\n",
        "    exe.run(star_program)\n",
        "\n",
        "    for pass_id in range(EPOCH_NUM):\n",
        "        progress_bar = tqdm(train_reader(), position=0, leave=True, total=round(buf_size/batch_size))\n",
        "        for step_id, data_train in enumerate(train_reader()):\n",
        "            avg_loss_value = exe.run(main_program,\n",
        "                                     feed=feeder.feed(data_train),\n",
        "                                     fetch_list=[avg_cost, acc])\n",
        "\n",
        "            progress_bar.set_description(f\"Epoch {step_id+1}/{EPOCH_NUM}\")\n",
        "            progress_bar.set_postfix(train_loss=np.round(avg_loss_value[0], 3))\n",
        "            progress_bar.update()\n",
        "\n",
        "        avg_cost_test, accuracy_test = train_test(test_program,\n",
        "                                                  reader=test_reader)\n",
        "\n",
        "        # save parameters\n",
        "        if params_dirname is not None:\n",
        "            fluid.io.save_inference_model(params_dirname, [\"pixel\"],\n",
        "                                          [predict], exe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-GHxnCUZsj1",
        "outputId": "4b335e2d-3475-4d9c-c99b-6f0afd0c055a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/2:  10%|█         | 41/391 [04:39<38:28,  6.60s/it, train_loss=[2.22]]"
          ]
        }
      ],
      "source": [
        "train_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YwmBBH5a-TV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}